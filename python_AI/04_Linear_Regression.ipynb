{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nDdR3N2R2PJ"
   },
   "source": [
    "<a href=\"https://practicalai.me\"><img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/rounded_logo.png\" width=\"100\" align=\"left\" hspace=\"20px\" vspace=\"20px\"></a>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/04_Linear_Regression/linear_regression.png\" width=\"150\" vspace=\"20px\" align=\"right\">\n",
    "\n",
    "<div align=\"left\">\n",
    "<h1>Linear Regression</h1>\n",
    "In this lesson we will learn about linear regression. We will understand the basic math behind it, implement it in Python. and then look at ways of interpreting the linear model.\n",
    "<br><p>\n",
    "<em>一元线性回归基本步骤：<br>\n",
    "数据提取➡️数据预处理➡️标准化➡️建模➡️训练➡️评估➡️图形展示➡️推断➡️方程解释</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Izx6GIn_SETR"
   },
   "source": [
    "<table align=\"center\">\n",
    "  <td>\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/rounded_logo.png\" width=\"25\"><a target=\"_blank\" href=\"https://practicalai.me\"> View on practicalAI</a>\n",
    "  </td>\n",
    "  <td>\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/colab_logo.png\" width=\"25\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/04_Linear_Regression.ipynb\"> Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/github_logo.png\" width=\"22\"><a target=\"_blank\" href=\"https://github.com/practicalAI/practicalAI/blob/master/notebooks/04_Linear_Regression.ipynb\"> View code on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoMq0eFRvugb"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LaFUStk-24I4"
   },
   "source": [
    "Our goal is to learn a linear model $\\hat{y}$ that models $y$ given $X$. \n",
    "\n",
    "$\\hat{y} = XW + b$\n",
    "* $\\hat{y}$ = predictions | $\\in \\mathbb{R}^{NX1}$ ($N$ is the number of samples)\n",
    "* $X$ = inputs | $\\in \\mathbb{R}^{NXD}$ ($D$ is the number of features)\n",
    "* $W$ = weights | $\\in \\mathbb{R}^{DX1}$ \n",
    "* $b$ = bias | $\\in \\mathbb{R}^{1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAgr7Grv9pb6"
   },
   "source": [
    "* **Objective:**  Use inputs $X$ to predict the output $\\hat{y}$ using a linear model. The model will be a line of best fit that minimizes the distance between the predicted (model's output) and target (ground truth) values. Training data $(X, y)$ is used to train the model and learn the weights $W$ using gradient descent.\n",
    "* **Advantages:**\n",
    "  * Computationally simple.\n",
    "  * Highly interpretable.\n",
    "  * Can account for continuous and categorical features.\n",
    "* **Disadvantages:**\n",
    "  * The model will perform well only when the data is linearly separable (for classification).\n",
    "  * Usually not used for classification and only for regression.\n",
    "* **Miscellaneous:** You can also use linear regression for binary classification tasks where if the predicted continuous value is above a threshold, it belongs to a certain class. But we will cover better techniques for classification in future lessons and will focus on linear regression for continuous regression tasks only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xP7XD24-09Io"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "476yPgTM1BKJ"
   },
   "source": [
    "1. Randomly initialize the model's weights $W$ (we'll cover more effective initalization strategies in future lessons).\n",
    "2. Feed inputs $X$ into the model to receive the predictions $\\hat{y}$.\n",
    "  * $\\hat{y} = XW + b$\n",
    "3. Compare the predictions $\\hat{y}$ with the actual target values $y$ using the objective (cost) function to determine the loss $J$. A common objective function for linear regression is mean squarred error (MSE). This function calculates the difference between the predicted and target values and squares it.\n",
    "\n",
    "  * $J(\\theta) = MSE = \\frac{1}{N} \\sum_i (y_i - \\hat{y}_i)^2 $\n",
    "    * ${y}$ = ground truth | $\\in \\mathbb{R}^{NX1}$\n",
    "    * $\\hat{y}$ = predictions | $\\in \\mathbb{R}^{NX1}$\n",
    "4. Calculate the gradient of loss $J(\\theta)$ w.r.t to the model weights.\n",
    "  * $J(\\theta) = \\frac{1}{N} \\sum_i (y_i - \\hat{y}_i)^2  = \\frac{1}{N}\\sum_i (y_i - X_iW)^2 $\n",
    "    * $\\frac{\\partial{J}}{\\partial{W}} = -\\frac{2}{N} \\sum_i (y_i - X_iW) X_i = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i) X_i$\n",
    "    * $\\frac{\\partial{J}}{\\partial{W}} = -\\frac{2}{N} \\sum_i (y_i - X_iW)1 = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i)1$\n",
    "5. Update the weights $W$ using a small learning rate $\\alpha$. The simplified intuition is that the gradient tells you the direction for how to increase something so subtracting it will help you go the other way since we want to decrease loss $J(\\theta)$:\n",
    "  * $W = W - \\alpha\\frac{\\partial{J}}{\\partial{W}}$\n",
    "  * $b = b - \\alpha\\frac{\\partial{J}}{\\partial{b}}$\n",
    "6. Repeat steps 2 - 5 to minimize the loss and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqEglvPJTifp"
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_style": "center",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ebK53z_VctPM",
    "outputId": "c8ebb86c-8df7-4465-957c-569c2595cbd1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "# Use TensorFlow 2.x\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade --ignore-installed tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HRXD7LqVJZ43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NFsKg-Z6IWqG"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "SEED = 1234\n",
    "SHUFFLE = True\n",
    "NUM_SAMPLES = 50\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "INPUT_DIM = 1\n",
    "HIDDEN_DIM = 1\n",
    "LEARNING_RATE = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "K8uaUq8XSsQ8"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducability\n",
    "np.random.seed(SEED)\n",
    "# tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvJKjkMeJP4Q"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RuPl9qlSJTIY"
   },
   "source": [
    "We're going to create some simple dummy data to apply linear regression on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "w5T7yGfiEQnx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_a--YWXdwig"
   },
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "McZ7wHsve61A"
   },
   "source": [
    "We're going to create some dummy data to train our linear regression model on. We're going to create roughly linear data (`y = 3.5X + 10`) with some random noise so the points don't all align in a straight line. Our goal is to have the model converge to a similar linear equation (there will be slight variance since we added some noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jNWcn3uadzpJ"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "def generate_data(num_samples):\n",
    "    \"\"\"Generate dummy data for linear regression.\"\"\"\n",
    "    X = np.array(range(num_samples))\n",
    "    random_noise = np.random.uniform(-10,20,size=num_samples)\n",
    "    y = 3.5*X + random_noise # add some noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ppGTtKRrdwgF"
   },
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4f9KSJR6flBt"
   },
   "source": [
    "Now let's load the data onto a dataframe and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2mb2SjSQIWvF",
    "outputId": "5a96430d-6776-4356-d685-2ce7fd7ddf26"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.254416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>12.163263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10.131832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>24.060758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>27.399274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X          y\n",
       "0  0.0  -4.254416\n",
       "1  1.0  12.163263\n",
       "2  2.0  10.131832\n",
       "3  3.0  24.060758\n",
       "4  4.0  27.399274"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random (linear) data\n",
    "X, y = generate_data(num_samples=NUM_SAMPLES)\n",
    "data = np.vstack([X, y]).T\n",
    "df = pd.DataFrame(data, columns=['X', 'y'])\n",
    "X = df[['X']].values\n",
    "y = df[['y']].values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "6LwVVOkiLfBN",
    "outputId": "ed7e32c6-c0e2-447c-ad0c-dc4e4d734597"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c9495c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "plt.title(\"Generated data\")\n",
    "plt.scatter(x=df['X'], y=df['y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qwn29SjK-XCg"
   },
   "source": [
    "# Split data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8Z0Nl1Oy3eOJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_bbSMUZ3e3m"
   },
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XAIAT-QoKn1"
   },
   "source": [
    "Since our task is a regression task, we will randomly split our dataset into **three** sets: train, validation and test data splits.\n",
    "\n",
    "* train: used to train our model.\n",
    "* val : used to validate our model's performance during training.\n",
    "* test: used to do an evaluation of our fully trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wIw3j4Z_0ya"
   },
   "source": [
    "<div align=\"left\">\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/lightbulb.gif\" width=\"45px\" align=\"left\" hspace=\"10px\">\n",
    "</div>\n",
    "Splitting the data for classification tasks are a bit different in that we want similar class distributions in each data split. We'll see this in action in the next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uKmBKodpgHEE"
   },
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, val_size, test_size, shuffle):\n",
    "    \"\"\"Split data into train/val/test datasets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, shuffle=shuffle)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=val_size, shuffle=shuffle)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2SFTiscx3gNi"
   },
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "WuUQwD72NVAE",
    "outputId": "2eb09cdd-d96a-4242-80d3-20222efa1e41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (35, 1), y_train: (35, 1)\n",
      "X_val: (7, 1), y_test: (7, 1)\n",
      "X_test: (8, 1), y_test: (8, 1)\n",
      "X_train[0]: [12.]\n",
      "y_train[0]: [52.50388806]\n"
     ]
    }
   ],
   "source": [
    "# Create data splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    X, y, val_size=VAL_SIZE, test_size=TEST_SIZE, shuffle=SHUFFLE)\n",
    "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print (f\"X_val: {X_val.shape}, y_test: {y_val.shape}\")\n",
    "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print (f\"X_train[0]: {X_train[0]}\")\n",
    "print (f\"y_train[0]: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-Eksj89d_yv"
   },
   "source": [
    "# Standardize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJVs6JF7trja"
   },
   "source": [
    "We need to standardize our data (zero mean and unit variance) in order to optimize quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yD7GFSnvsFfg"
   },
   "source": [
    "$z = \\frac{x_i - \\mu}{\\sigma}$\n",
    "* $z$ = standardized value\n",
    "* $x_i$ = inputs\n",
    "* $\\mu$ = mean\n",
    "* $\\sigma$ = standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VlOYPD5GRjRC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CiE3oLCkOCEa"
   },
   "outputs": [],
   "source": [
    "# Standardize the data (mean=0, std=1) using training data\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "E7dKUyWGJ4Av"
   },
   "outputs": [],
   "source": [
    "# Apply scaler on training and test data and val data\n",
    "standardized_X_train = X_scaler.transform(X_train)\n",
    "standardized_y_train = y_scaler.transform(y_train).ravel()\n",
    "standardized_X_val = X_scaler.transform(X_val)\n",
    "standardized_y_val = y_scaler.transform(y_val).ravel()\n",
    "standardized_X_test = X_scaler.transform(X_test)\n",
    "standardized_y_test = y_scaler.transform(y_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "3JC-YFYFJ39Z",
    "outputId": "3cfb0813-841b-4640-bbbf-4715e5afab20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized_X_train: mean: -1.2688263138573217e-16, std: 1.0\n",
      "standardized_y_train: mean: 8.961085841617335e-17, std: 1.0\n",
      "standardized_X_val: mean: 0.455275148918323, std: 0.8941749819083381\n",
      "standardized_y_val: mean: 0.48468545395300805, std: 0.9785039908565315\n",
      "standardized_X_test: mean: -0.44282621906508784, std: 1.0652341142978086\n",
      "standardized_y_test: mean: -0.4940327711822252, std: 1.0650188983159736\n"
     ]
    }
   ],
   "source": [
    "# Check (means should be ~0 and std should be ~1)\n",
    "print (f\"standardized_X_train: mean: {np.mean(standardized_X_train, axis=0)[0]}, std: {np.std(standardized_X_train, axis=0)[0]}\")\n",
    "print (f\"standardized_y_train: mean: {np.mean(standardized_y_train, axis=0)}, std: {np.std(standardized_y_train, axis=0)}\")\n",
    "print (f\"standardized_X_val: mean: {np.mean(standardized_X_val, axis=0)[0]}, std: {np.std(standardized_X_val, axis=0)[0]}\")\n",
    "print (f\"standardized_y_val: mean: {np.mean(standardized_y_val, axis=0)}, std: {np.std(standardized_y_val, axis=0)}\")\n",
    "print (f\"standardized_X_test: mean: {np.mean(standardized_X_test, axis=0)[0]}, std: {np.std(standardized_X_test, axis=0)[0]}\")\n",
    "print (f\"standardized_y_test: mean: {np.mean(standardized_y_test, axis=0)}, std: {np.std(standardized_y_test, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sdruDHf_laWg"
   },
   "source": [
    "# From scratch\n",
    "\n",
    "Before we use TensorFlow 2.0 + Keras we will implent linear regression from scratch using NumPy so we can:\n",
    "1. Absorb the fundamental concepts by implementing from scratch\n",
    "2. Appreciate the level of abstraction TensorFlow provides\n",
    "\n",
    "<div align=\"left\">\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/lightbulb.gif\" width=\"45px\" align=\"left\" hspace=\"10px\">\n",
    "</div>\n",
    "\n",
    "It's normal to find the math and code in this section slightly complex. You can still read each of the steps to build intuition for when we implement this using TensorFlow + Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "U4Prl8dDl-T4",
    "outputId": "55ef92c1-4e73-4a63-ff51-9d3965cf5bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (35, 1)\n",
      "y: (35, 1)\n"
     ]
    }
   ],
   "source": [
    "#维度X,y保持一致\n",
    "standardized_y_train = standardized_y_train.reshape(-1, 1)\n",
    "print (f\"X: {standardized_X_train.shape}\")\n",
    "print (f\"y: {standardized_y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8vzjUSW-05x"
   },
   "source": [
    "Our goal is to learn a linear model $\\hat{y}$ that models $y$ given $X$. \n",
    "\n",
    "$\\hat{y} = XW + b$\n",
    "* $\\hat{y}$ = predictions | $\\in \\mathbb{R}^{NX1}$ ($N$ is the number of samples)\n",
    "* $X$ = inputs | $\\in \\mathbb{R}^{NXD}$ ($D$ is the number of features)\n",
    "* $W$ = weights | $\\in \\mathbb{R}^{DX1}$ \n",
    "* $b$ = bias | $\\in \\mathbb{R}^{1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1QzuBFM8by6"
   },
   "source": [
    "1.   Randomly initialize the model's weights $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "KJrD1GiCl-bX",
    "outputId": "43c0157f-47a3-4099-d400-50062b6e45b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: (1, 1)\n",
      "b: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize random weights\n",
    "W = 0.01 * np.random.randn(INPUT_DIM, 1)\n",
    "b = np.zeros((1, 1))\n",
    "print (f\"W: {W.shape}\")\n",
    "print (f\"b: {b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IptdjlS8sBw"
   },
   "source": [
    "2. Feed inputs $X$ into the model to receive the predictions $\\hat{y}$.\n",
    "  * $\\hat{y} = XW + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "_RQsyPW4sSLb",
    "outputId": "449589ec-558f-4925-8621-ce9f652a97c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: (35, 1)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass [NX1] · [1X1] = [NX1]\n",
    "y_hat = np.dot(standardized_X_train, W) + b\n",
    "print (f\"y_hat: {y_hat.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5ZTGKol84VO"
   },
   "source": [
    "3. Compare the predictions $\\hat{y}$ with the actual target values $y$ using the objective (cost) function to determine the loss $J$. A common objective function for linear regression is mean squarred error (MSE). This function calculates the difference between the predicted and target values and squares it.\n",
    "\n",
    "  * $J(\\theta) = MSE = \\frac{1}{N} \\sum_{i-1}^{N} (y_i - \\hat{y}_i)^2 $\n",
    "    * ${y}$ = ground truth | $\\in \\mathbb{R}^{NX1}$\n",
    "    * $\\hat{y}$ = predictions | $\\in \\mathbb{R}^{NX1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "Cqs_50xcsSkm",
    "outputId": "eb89f7c8-63c5-49b3-e94e-654519728168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.02\n"
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "N = len(standardized_y_train)\n",
    "loss = (1/N) * np.sum((standardized_y_train - y_hat)**2)\n",
    "print (f\"loss: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBQ59qNs90-u"
   },
   "source": [
    "4. Calculate the gradient of loss $J(\\theta)$ w.r.t to the model weights.\n",
    "  * $J(\\theta) = \\frac{1}{N} \\sum_i (y_i - \\hat{y}_i)^2  = \\frac{1}{N}\\sum_i (y_i - X_iW)^2 $\n",
    "    * $\\frac{\\partial{J}}{\\partial{W}} = -\\frac{2}{N} \\sum_i (y_i - X_iW) X_i = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i) X_i$\n",
    "    * $\\frac{\\partial{J}}{\\partial{W}} = -\\frac{2}{N} \\sum_i (y_i - X_iW)1 = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i)1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NlN9F8bysSiP"
   },
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "dW = -(2/N) * np.sum((standardized_y_train - y_hat) * standardized_X_train)\n",
    "db = -(2/N) * np.sum((standardized_y_train - y_hat) * 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vA8K64TF_fRD"
   },
   "source": [
    "5. Update the weights $W$ using a small learning rate $\\alpha$. The simplified intuition is that the gradient tells you the direction for how to increase something so subtracting it will help you go the other way since we want to decrease loss $J(\\theta)$:\n",
    "  * $W = W - \\alpha\\frac{\\partial{J}}{\\partial{W}}$\n",
    "  * $b = b - \\alpha\\frac{\\partial{J}}{\\partial{b}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nBa96kSXvDnJ"
   },
   "outputs": [],
   "source": [
    "# Update weights\n",
    "W += -LEARNING_RATE * dW\n",
    "b += -LEARNING_RATE * db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3b0P4Ls6_thq"
   },
   "source": [
    "6. Repeat steps 2 - 5 to minimize the loss and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "xF6_RVuul-ZJ",
    "outputId": "d6dc9f2a-e5ad-465a-d8ed-dd8098bb836a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.025\n",
      "Epoch: 10, loss: 0.040\n",
      "Epoch: 20, loss: 0.028\n",
      "Epoch: 30, loss: 0.028\n",
      "Epoch: 40, loss: 0.028\n",
      "Epoch: 50, loss: 0.028\n",
      "Epoch: 60, loss: 0.028\n",
      "Epoch: 70, loss: 0.028\n",
      "Epoch: 80, loss: 0.028\n",
      "Epoch: 90, loss: 0.028\n"
     ]
    }
   ],
   "source": [
    "# Initialize random weights\n",
    "W = 0.01 * np.random.randn(INPUT_DIM, 1)\n",
    "b = np.zeros((1, ))\n",
    "\n",
    "# Training loop\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "\n",
    "    # Forward pass [NX1] · [1X1] = [NX1]\n",
    "    y_hat = np.dot(standardized_X_train, W) + b\n",
    "\n",
    "    # Loss\n",
    "    loss = (1/len(standardized_y_train)) * np.sum((standardized_y_train - y_hat)**2)\n",
    "\n",
    "    # show progress\n",
    "    if epoch_num%10 == 0:\n",
    "        print (f\"Epoch: {epoch_num}, loss: {loss:.3f}\")\n",
    "\n",
    "    # Backpropagation\n",
    "    dW = -(2/N) * np.sum((standardized_y_train - y_hat) * standardized_X_train)\n",
    "    db = -(2/N) * np.sum((standardized_y_train - y_hat) * 1)\n",
    "\n",
    "    # Update weights\n",
    "    W += -LEARNING_RATE * dW\n",
    "    b += -LEARNING_RATE * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fcIUx6d-69_g"
   },
   "outputs": [],
   "source": [
    "# Predictions \n",
    "pred_train = W*standardized_X_train + b\n",
    "pred_test = W*standardized_X_test + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "WB_c9ek16-FC",
    "outputId": "2b9b8568-6e6d-48ec-98d9-cb48fedda2dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_MSE: 0.03, test_MSE: 2.24\n"
     ]
    }
   ],
   "source": [
    "# Train and test MSE\n",
    "train_mse = np.mean((standardized_y_train - pred_train) ** 2)\n",
    "test_mse = np.mean((standardized_y_test - pred_test) ** 2)\n",
    "print (f\"train_MSE: {train_mse:.2f}, test_MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "by9uqJbC699J",
    "outputId": "52652814-cfa3-4d39-8d58-a085c2cab0ea"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAE/CAYAAAAKZVEAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ2ZmZmZmVjgP7MzMzMzMzAr3ANc3WzyY7XUYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1cbdeac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# Plot train data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plt.scatter(standardized_X_train, standardized_y_train, label='y_train')\n",
    "plt.plot(standardized_X_train, pred_train, color='red', linewidth=1, linestyle='-', label='model')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Plot test data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plt.scatter(standardized_X_test, standardized_y_test, label='y_test')\n",
    "plt.plot(standardized_X_test, pred_test, color='red', linewidth=1, linestyle='-', label='model')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3MlFkZ8_der"
   },
   "source": [
    "Since we standardized our inputs and outputs, our weights were fit to those standardized values. So we need to unstandardize our weights so we can compare it to our true weight (3.5).\n",
    "\n",
    "Note that both X and y were standardized.\n",
    "\n",
    "$\\hat{y}_{scaled} = b_{scaled} + \\sum_{j=1}^{k}W_{{scaled}_j}x_{{scaled}_j}$\n",
    "* $y_{scaled} = \\frac{\\hat{y} - \\bar{y}}{\\sigma_y}$\n",
    "* $x_{scaled} = \\frac{x_j - \\bar{x}_j}{\\sigma_j}$\n",
    "\n",
    "$\\frac{\\hat{y} - \\bar{y}}{\\sigma_y} = b_{scaled} + \\sum_{j=1}^{k}W_{{scaled}_j}\\frac{x_j - \\bar{x}_j}{\\sigma_j}$\n",
    "\n",
    "$ \\hat{y}_{scaled} = \\frac{\\hat{y}_{unscaled} - \\bar{y}}{\\sigma_y} = {b_{scaled}} + \\sum_{j=1}^{k} {W}_{{scaled}_j} (\\frac{x_j - \\bar{x}_j}{\\sigma_j}) $\n",
    "\n",
    "$\\hat{y}_{unscaled} = b_{scaled}\\sigma_y + \\bar{y} - \\sum_{j=1}^{k} {W}_{{scaled}_j}(\\frac{\\sigma_y}{\\sigma_j})\\bar{x}_j + \\sum_{j=1}^{k}{W}_{{scaled}_j}(\\frac{\\sigma_y}{\\sigma_j})x_j $\n",
    "\n",
    "In the expression above, we can see the expression $\\hat{y}_{unscaled} = W_{unscaled}x + b_{unscaled} $ where\n",
    "\n",
    "* $W_{unscaled} = \\sum_{j=1}^{k}{W}_j(\\frac{\\sigma_y}{\\sigma_j}) $\n",
    "\n",
    "* $b_{unscaled} = b_{scaled}\\sigma_y + \\bar{y} - \\sum_{j=1}^{k} {W}_j(\\frac{\\sigma_y}{\\sigma_j})\\bar{x}_j$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "C-PcjG4G2TRq",
    "outputId": "74ac88ec-1c1a-4747-bd82-7902f9318bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[actual] y = 3.5X + noise\n",
      "[model] y_hat = 3.4X + 9.5\n"
     ]
    }
   ],
   "source": [
    "# Unscaled weights\n",
    "W_unscaled = W * (y_scaler.scale_/X_scaler.scale_)\n",
    "b_unscaled = b * y_scaler.scale_ + y_scaler.mean_ - np.sum(W_unscaled*X_scaler.mean_)\n",
    "print (\"[actual] y = 3.5X + noise\")\n",
    "print (f\"[model] y_hat = {W_unscaled[0][0]:.1f}X + {b_unscaled[0]:.1f}\")   #实际中只关心这一条"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LD5m4v1A9nqF"
   },
   "source": [
    "Now let's implement linear regression with TensorFlow + Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "33NIijOMKqZF"
   },
   "source": [
    "# TensorFlow + Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCuIxAH7KuwG"
   },
   "source": [
    "We will be using [Dense layers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense) in our MLP implementation. The layer applies an activation function on the dot product of the layer's inputs and its weights.\n",
    "\n",
    "$ z = \\text{activation}(XW)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QvjSlcsfKqjY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/SJX/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "07hfw6dcK_jo",
    "outputId": "c05d40b6-a1ec-4bfc-b72f-c102fde37ecd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0109 18:04:04.206878 4647886272 deprecation.py:506] From /Users/SJX/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z (?, 1) = x (?, 1) · W (1, 1) + b (1,)\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=(INPUT_DIM,))\n",
    "fc = Dense(units=HIDDEN_DIM, activation='linear')\n",
    "z = fc(x)\n",
    "W, b = fc.weights\n",
    "print (f\"z {z.shape} = x {x.shape} · W {W.shape} + b {b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense/bias:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4drnbzryeVsD"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sGH_pQaDOb49"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import MeanAbsolutePercentageError\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "V-3zRdoIeaas"
   },
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "class LinearRegression(Model):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.fc1 = Dense(units=hidden_dim, activation='linear')\n",
    "        \n",
    "    def call(self, x_in, training=False):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        y_pred = self.fc1(x_in)\n",
    "        return y_pred\n",
    "    \n",
    "    def sample(self, input_shape):\n",
    "        x_in = Input(shape=input_shape)\n",
    "        return Model(inputs=x_in, outputs=self.call(x_in)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "8y3YIttkeaX2",
    "outputId": "c1b26629-a5b8-47bc-c446-4a85de7e2c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = LinearRegression(hidden_dim=HIDDEN_DIM)\n",
    "model.sample(input_shape=(INPUT_DIM,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yj5snqjN0Y4j"
   },
   "source": [
    "When we implemented linear regression from scratch, we used batch gradient descent to update our weights. But there are actually many different [gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/) to choose from and it depends on the situation. However, the [ADAM optimizer](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms/#adam) has become a standard algorithm for most cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0eZiOOYeerCV"
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(optimizer=Adam(lr=LEARNING_RATE),\n",
    "              loss=MeanSquaredError(),\n",
    "              metrics=[MeanAbsolutePercentageError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysKlKW_lki9R"
   },
   "source": [
    "<img height=\"45\" src=\"http://bestanimations.com/HomeOffice/Lights/Bulbs/animated-light-bulb-gif-29.gif\" align=\"left\" vspace=\"5px\" hspace=\"10px\">\n",
    "\n",
    "Here are the full list of options for [optimizer](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers), [loss](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses) and [metrics](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VtxXsNq3hrz"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWn-wFPEFu38"
   },
   "source": [
    "When we implemented linear regression from scratch, we used batch gradient descent to update our weights. This means that we calculated the gradients using the entire training dataset. We also could've updated our weights using stochastic gradient descent (SGD) where we pass in one training example at a time. The current standard is mini-batch gradient descent, which strikes a balance between batch and stochastic GD, where we update the weights using a mini-batch of n (`BATCH_SIZE`) samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "u_ZOiIGleq_l",
    "outputId": "81046735-603d-48ce-abc3-0c4b42b88ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35 samples, validate on 7 samples\n",
      "Epoch 1/100\n",
      "35/35 [==============================] - 0s 3ms/sample - loss: 5.9274 - mean_absolute_percentage_error: 298.8128 - val_loss: 5.3682 - val_mean_absolute_percentage_error: 181.5307\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 0s 545us/sample - loss: 4.0715 - mean_absolute_percentage_error: 243.9709 - val_loss: 3.5870 - val_mean_absolute_percentage_error: 155.4494\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 0s 431us/sample - loss: 2.6682 - mean_absolute_percentage_error: 180.7810 - val_loss: 2.1894 - val_mean_absolute_percentage_error: 131.2296\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 0s 359us/sample - loss: 1.6260 - mean_absolute_percentage_error: 124.9877 - val_loss: 1.2119 - val_mean_absolute_percentage_error: 108.0205\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 0s 400us/sample - loss: 0.9058 - mean_absolute_percentage_error: 103.0718 - val_loss: 0.5986 - val_mean_absolute_percentage_error: 85.8562\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 0s 229us/sample - loss: 0.4497 - mean_absolute_percentage_error: 86.3067 - val_loss: 0.2574 - val_mean_absolute_percentage_error: 65.1962\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 0s 450us/sample - loss: 0.1922 - mean_absolute_percentage_error: 68.7496 - val_loss: 0.0938 - val_mean_absolute_percentage_error: 46.7636\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 0s 376us/sample - loss: 0.0715 - mean_absolute_percentage_error: 53.4390 - val_loss: 0.0306 - val_mean_absolute_percentage_error: 31.3600\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 0s 246us/sample - loss: 0.0351 - mean_absolute_percentage_error: 44.2534 - val_loss: 0.0152 - val_mean_absolute_percentage_error: 22.6407\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 0s 246us/sample - loss: 0.0405 - mean_absolute_percentage_error: 44.3346 - val_loss: 0.0169 - val_mean_absolute_percentage_error: 17.6894\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 0s 397us/sample - loss: 0.0570 - mean_absolute_percentage_error: 45.1011 - val_loss: 0.0203 - val_mean_absolute_percentage_error: 16.4284\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 0s 272us/sample - loss: 0.0666 - mean_absolute_percentage_error: 45.2072 - val_loss: 0.0202 - val_mean_absolute_percentage_error: 16.4294\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 0s 266us/sample - loss: 0.0640 - mean_absolute_percentage_error: 44.2797 - val_loss: 0.0174 - val_mean_absolute_percentage_error: 16.1413\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 0s 464us/sample - loss: 0.0532 - mean_absolute_percentage_error: 42.8047 - val_loss: 0.0149 - val_mean_absolute_percentage_error: 16.8430\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 0s 309us/sample - loss: 0.0411 - mean_absolute_percentage_error: 41.3964 - val_loss: 0.0150 - val_mean_absolute_percentage_error: 19.3464\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 0s 321us/sample - loss: 0.0325 - mean_absolute_percentage_error: 40.1400 - val_loss: 0.0181 - val_mean_absolute_percentage_error: 22.1388\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 0s 517us/sample - loss: 0.0287 - mean_absolute_percentage_error: 39.0937 - val_loss: 0.0232 - val_mean_absolute_percentage_error: 25.2355\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 0s 474us/sample - loss: 0.0283 - mean_absolute_percentage_error: 38.1845 - val_loss: 0.0286 - val_mean_absolute_percentage_error: 27.2888\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0334 - mean_absolute_percentage_error: 27.908 - 0s 359us/sample - loss: 0.0293 - mean_absolute_percentage_error: 37.1865 - val_loss: 0.0329 - val_mean_absolute_percentage_error: 28.3460\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 0s 426us/sample - loss: 0.0303 - mean_absolute_percentage_error: 36.3563 - val_loss: 0.0353 - val_mean_absolute_percentage_error: 28.6137\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 0s 358us/sample - loss: 0.0307 - mean_absolute_percentage_error: 35.7808 - val_loss: 0.0355 - val_mean_absolute_percentage_error: 28.3553\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 0s 493us/sample - loss: 0.0305 - mean_absolute_percentage_error: 35.5851 - val_loss: 0.0339 - val_mean_absolute_percentage_error: 27.8141\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 0s 381us/sample - loss: 0.0299 - mean_absolute_percentage_error: 35.7399 - val_loss: 0.0315 - val_mean_absolute_percentage_error: 27.1729\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0333 - mean_absolute_percentage_error: 29.340 - 0s 298us/sample - loss: 0.0292 - mean_absolute_percentage_error: 36.1041 - val_loss: 0.0291 - val_mean_absolute_percentage_error: 26.5498\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 0s 244us/sample - loss: 0.0288 - mean_absolute_percentage_error: 36.5507 - val_loss: 0.0272 - val_mean_absolute_percentage_error: 26.0136\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 0s 376us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.8763 - val_loss: 0.0259 - val_mean_absolute_percentage_error: 25.6027\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 0s 422us/sample - loss: 0.0284 - mean_absolute_percentage_error: 37.0349 - val_loss: 0.0253 - val_mean_absolute_percentage_error: 25.3369\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 0s 303us/sample - loss: 0.0283 - mean_absolute_percentage_error: 37.0302 - val_loss: 0.0253 - val_mean_absolute_percentage_error: 25.2201\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0341 - mean_absolute_percentage_error: 30.567 - 0s 403us/sample - loss: 0.0283 - mean_absolute_percentage_error: 36.9310 - val_loss: 0.0255 - val_mean_absolute_percentage_error: 25.2374\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 0s 339us/sample - loss: 0.0283 - mean_absolute_percentage_error: 36.8190 - val_loss: 0.0259 - val_mean_absolute_percentage_error: 25.3547\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 0s 396us/sample - loss: 0.0284 - mean_absolute_percentage_error: 36.7548 - val_loss: 0.0263 - val_mean_absolute_percentage_error: 25.5247\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 0s 251us/sample - loss: 0.0284 - mean_absolute_percentage_error: 36.7334 - val_loss: 0.0267 - val_mean_absolute_percentage_error: 25.6974\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 0s 245us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7361 - val_loss: 0.0269 - val_mean_absolute_percentage_error: 25.8335\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 0s 467us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7396 - val_loss: 0.0271 - val_mean_absolute_percentage_error: 25.9123\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 0s 269us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7315 - val_loss: 0.0271 - val_mean_absolute_percentage_error: 25.9335\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 0s 291us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7137 - val_loss: 0.0271 - val_mean_absolute_percentage_error: 25.9117\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 0s 660us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.6973 - val_loss: 0.0271 - val_mean_absolute_percentage_error: 25.8680\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 0s 318us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.6923 - val_loss: 0.0270 - val_mean_absolute_percentage_error: 25.8212\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 0s 628us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7013 - val_loss: 0.0269 - val_mean_absolute_percentage_error: 25.7834\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 0s 530us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7189 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7593\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 0s 382us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7359 - val_loss: 0.0267 - val_mean_absolute_percentage_error: 25.7482\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 824us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7453 - val_loss: 0.0267 - val_mean_absolute_percentage_error: 25.7470\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 0s 769us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7453 - val_loss: 0.0267 - val_mean_absolute_percentage_error: 25.7520\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 0s 802us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7388 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7604\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 0s 607us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7308 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7698\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 0s 721us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7252 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7782\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0338 - mean_absolute_percentage_error: 30.179 - 0s 870us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7233 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7839\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 0s 539us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7239 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7861\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 0s 670us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7251 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7849\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 0s 739us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7257 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7816\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 0s 850us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7255 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7776\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 0s 807us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7251 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7742\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 0s 736us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7250 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7720\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 0s 833us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7253 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7713\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 0s 574us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7257 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7715\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 0s 587us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7257 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7720\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 0s 529us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7253 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7726\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 0s 580us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7245 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7730\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 0s 492us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7238 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7730\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 0s 591us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7233 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7728\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 0s 430us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7231 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7724\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 0s 452us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7230 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7718\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 0s 547us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7230 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7712\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 0s 460us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7228 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7706\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 0s 323us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7225 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7701\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 0s 417us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7223 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7698\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 0s 247us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7220 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7695\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 0s 335us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7218 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7693\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 0s 316us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7215 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7691\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 0s 371us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7213 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7688\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 0s 476us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7210 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7685\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 0s 461us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7207 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7681\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 0s 399us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7205 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7678\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 0s 355us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7203 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7674\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 0s 369us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7200 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7671\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 0s 528us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7198 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7668\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 0s 464us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7196 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7665\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 0s 506us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7193 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7661\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 0s 502us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7191 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7658\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 0s 367us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7189 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7655\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 0s 317us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7186 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7652\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 0s 650us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7184 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7649\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 0s 423us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7182 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7646\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 393us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7179 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7643\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 0s 356us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7177 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7640\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 0s 320us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7175 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7637\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 0s 269us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7172 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7634\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 0s 353us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7170 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7631\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 0s 352us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7168 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7628\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 0s 326us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7165 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7625\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 0s 353us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7163 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7622\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 0s 367us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7161 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7619\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 0s 483us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7159 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7616\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 0s 495us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7156 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7613\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 0s 504us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7154 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7610\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 0s 559us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7152 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7607\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 0s 333us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7149 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7604\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 0s 560us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7147 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7601\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 0s 435us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7145 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7598\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 0s 355us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7143 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a31a9e9e8>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(x=standardized_X_train, \n",
    "          y=standardized_y_train,\n",
    "          validation_data=(standardized_X_val, standardized_y_val),\n",
    "          epochs=NUM_EPOCHS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          shuffle=False,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRl3Gvu8ewD4"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhdu9lBie_t_"
   },
   "source": [
    "There are several evaluation techniques to see how well our model performed. A common one for linear regression is mean squarred error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BZl9Jz8qetzI"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "pred_train = model.predict(standardized_X_train)\n",
    "pred_test = model.predict(standardized_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UT30FvMUetwe",
    "outputId": "24168a89-3644-4583-9a39-72b791f776be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_MSE: 0.03, test_MSE: 2.21\n"
     ]
    }
   ],
   "source": [
    "# Train and test MSE\n",
    "train_mse = np.mean((standardized_y_train - pred_train) ** 2)\n",
    "test_mse = np.mean((standardized_y_test - pred_test) ** 2)\n",
    "print (f\"train_MSE: {train_mse:.2f}, test_MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TegkJM2-YKEq"
   },
   "source": [
    "Since we only have one feature, it's easy to visually inspect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "I8nuCOjGeySz",
    "outputId": "f3cca3ea-7428-4ce3-e3d6-8f5e0487f54f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAE/CAYAAAAKZVEAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a31f7b2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# Plot train data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plt.scatter(standardized_X_train, standardized_y_train, label='y_train')\n",
    "plt.plot(standardized_X_train, pred_train, color='red', linewidth=1, linestyle='-', label='model')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Plot test data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plt.scatter(standardized_X_test, standardized_y_test, label='y_test')\n",
    "plt.plot(standardized_X_test, pred_test, color='red', linewidth=1, linestyle='-', label='model')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T621buxbe0VQ"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4W84oQzfNn_"
   },
   "source": [
    "After training a model, we can use it to predict on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "O_ZKW-p2e0fN"
   },
   "outputs": [],
   "source": [
    "# Feed in your own inputs\n",
    "sample_indices = [10, 15, 25]\n",
    "X_infer = np.array(sample_indices, dtype=np.float32)\n",
    "standardized_X_infer = X_scaler.transform(X_infer.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZjQ54zKazbE"
   },
   "source": [
    "Recall that we need to unstandardize our predictions.\n",
    "\n",
    "$ \\hat{y}_{scaled} = \\frac{\\hat{y} - \\mu_{\\hat{y}}}{\\sigma_{\\hat{y}}} $\n",
    "\n",
    "$ \\hat{y} = \\hat{y}_{scaled} * \\sigma_{\\hat{y}} + \\mu_{\\hat{y}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PMAdz712aE34",
    "outputId": "aed82cf8-86fe-4fc0-8971-884b8ca27b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.73 (actual) → 43.53 (predicted)\n",
      "59.34 (actual) → 60.06 (predicted)\n",
      "97.04 (actual) → 93.11 (predicted)\n"
     ]
    }
   ],
   "source": [
    "# Unstandardize predictions\n",
    "pred_infer = model.predict(standardized_X_infer) * np.sqrt(y_scaler.var_) + y_scaler.mean_\n",
    "for i, index in enumerate(sample_indices):\n",
    "    print (f\"{df.iloc[index]['y']:.2f} (actual) → {pred_infer[i][0]:.2f} (predicted)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43.53112316],\n",
       "       [60.0567312 ],\n",
       "       [93.10794058]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5CB4zRFe37l"
   },
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OhXo8CbPBZ-G"
   },
   "source": [
    "Linear regression offers the great advantage of being highly interpretable. Each feature has a coefficient which signifies it's importance/impact on the output variable y. We can interpret our coefficient as follows: by increasing X by 1 unit, we increase y by $W$ (~3.65) units. \n",
    "\n",
    "**Note**: Since we standardized our inputs and outputs for gradient descent, we need to apply an operation to our coefficients and intercept to interpret them. See proof in the `From scratch` section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lJOOjjLze6_U",
    "outputId": "2a735554-e3cd-42bc-db32-c83b0cc8ea8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[actual] y = 3.5X + noise\n",
      "[model] y_hat = 3.3X + 10.5\n"
     ]
    }
   ],
   "source": [
    "# Unstandardize coefficients (proof is in the `From Scratch` section above)\n",
    "W = model.layers[0].get_weights()[0][0][0]\n",
    "b = model.layers[0].get_weights()[1][0]\n",
    "W_unscaled = W * (y_scaler.scale_/X_scaler.scale_)\n",
    "b_unscaled = b * y_scaler.scale_ + y_scaler.mean_ - np.sum(W_unscaled*X_scaler.mean_)\n",
    "print (\"[actual] y = 3.5X + noise\")\n",
    "print (f\"[model] y_hat = {W_unscaled[0]:.1f}X + {b_unscaled[0]:.1f}\") #实际中只关心这一条就可以"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rToCXKqeJcvj"
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4GFv8xRJmOZ"
   },
   "source": [
    "Regularization helps decrease overfitting. Below is L2 regularization (ridge regression). There are many forms of regularization but they all work to reduce overfitting in our models. With L2 regularization, we are penalizing the weights with large magnitudes by decaying them. Having certain weights with high magnitudes will lead to preferential bias with the inputs and we want the model to work with all the inputs and not just a select few. There are also other types of regularization like L1 (lasso regression) which is useful for creating sparse models where some feature cofficients are zeroed out, or elastic which combines L1 and L2 penalties. \n",
    "\n",
    "**Note**: Regularization is not just for linear regression. You can use it to regularize any model's weights including the ones we will look at in future lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_OcpRxF-Oj7"
   },
   "source": [
    "$ J(\\theta) = = \\frac{1}{2}\\sum_{i}(X_iW - y_i)^2 + \\frac{\\lambda}{2}W^TW$\n",
    "\n",
    "$ \\frac{\\partial{J}}{\\partial{W}}  = X (\\hat{y} - y) + \\lambda W $\n",
    "\n",
    "$W = W- \\alpha\\frac{\\partial{J}}{\\partial{W}}$\n",
    "* $\\lambda$ is the regularzation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HHaazL9f8QZX"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VTIUZLbGZP4e"
   },
   "outputs": [],
   "source": [
    "L2_LAMBDA = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ORwkUqcuZhbX"
   },
   "outputs": [],
   "source": [
    "# Linear model with L2 regularization\n",
    "class LinearRegressionL2Regularization(Model):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(LinearRegressionL2Regularization, self).__init__()\n",
    "        self.fc1 = Dense(units=hidden_dim, activation='linear',\n",
    "                        kernel_regularizer=l2(l=L2_LAMBDA))\n",
    "        \n",
    "    def call(self, x_in, training=False):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        y_pred = self.fc1(x_in)\n",
    "        return y_pred\n",
    "    \n",
    "    def sample(self, input_shape):\n",
    "        x_in = Input(shape=input_shape)\n",
    "        return Model(inputs=x_in, outputs=self.call(x_in)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "IWCvYxBxZhd5",
    "outputId": "3670f885-252f-49fd-afc1-8c64a9328003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = LinearRegressionL2Regularization(hidden_dim=HIDDEN_DIM)\n",
    "model.sample(input_shape=(INPUT_DIM,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DJ-XpSYAoNBX"
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(optimizer=Adam(lr=LEARNING_RATE),\n",
    "              loss=MeanSquaredError(),\n",
    "              metrics=[MeanAbsolutePercentageError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Xlt9IuaCoM-X",
    "outputId": "f771fbe4-a860-42cc-8550-a6f7d1e42a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35 samples, validate on 7 samples\n",
      "Epoch 1/100\n",
      "35/35 [==============================] - 0s 3ms/sample - loss: 3.7703 - mean_absolute_percentage_error: 190.5097 - val_loss: 3.0525 - val_mean_absolute_percentage_error: 153.2347\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 0s 435us/sample - loss: 2.4137 - mean_absolute_percentage_error: 153.8904 - val_loss: 1.8712 - val_mean_absolute_percentage_error: 122.4003\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 0s 294us/sample - loss: 1.3518 - mean_absolute_percentage_error: 117.6363 - val_loss: 1.0143 - val_mean_absolute_percentage_error: 94.3467\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 0s 444us/sample - loss: 0.6320 - mean_absolute_percentage_error: 78.5035 - val_loss: 0.4725 - val_mean_absolute_percentage_error: 68.1107\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 0s 638us/sample - loss: 0.2671 - mean_absolute_percentage_error: 50.4852 - val_loss: 0.1749 - val_mean_absolute_percentage_error: 44.6023\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 0s 386us/sample - loss: 0.0790 - mean_absolute_percentage_error: 31.7839 - val_loss: 0.0471 - val_mean_absolute_percentage_error: 27.5702\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 0s 502us/sample - loss: 0.0402 - mean_absolute_percentage_error: 36.3313 - val_loss: 0.0266 - val_mean_absolute_percentage_error: 19.1009\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 0s 295us/sample - loss: 0.0636 - mean_absolute_percentage_error: 47.6728 - val_loss: 0.0434 - val_mean_absolute_percentage_error: 19.1558\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 0s 374us/sample - loss: 0.0945 - mean_absolute_percentage_error: 54.4244 - val_loss: 0.0547 - val_mean_absolute_percentage_error: 19.4144\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 0s 455us/sample - loss: 0.1050 - mean_absolute_percentage_error: 57.4592 - val_loss: 0.0520 - val_mean_absolute_percentage_error: 19.7060\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 0s 632us/sample - loss: 0.0935 - mean_absolute_percentage_error: 55.3321 - val_loss: 0.0400 - val_mean_absolute_percentage_error: 19.8152\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 0s 561us/sample - loss: 0.0721 - mean_absolute_percentage_error: 49.2159 - val_loss: 0.0275 - val_mean_absolute_percentage_error: 19.5635\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 0s 672us/sample - loss: 0.0523 - mean_absolute_percentage_error: 42.5658 - val_loss: 0.0272 - val_mean_absolute_percentage_error: 20.3108\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 0s 995us/sample - loss: 0.0410 - mean_absolute_percentage_error: 37.4619 - val_loss: 0.0356 - val_mean_absolute_percentage_error: 23.5599\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 0s 471us/sample - loss: 0.0392 - mean_absolute_percentage_error: 35.1118 - val_loss: 0.0449 - val_mean_absolute_percentage_error: 26.5672\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 0s 485us/sample - loss: 0.0400 - mean_absolute_percentage_error: 33.6730 - val_loss: 0.0479 - val_mean_absolute_percentage_error: 29.1549\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 0s 607us/sample - loss: 0.0406 - mean_absolute_percentage_error: 35.3990 - val_loss: 0.0470 - val_mean_absolute_percentage_error: 29.9674\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 0s 309us/sample - loss: 0.0404 - mean_absolute_percentage_error: 36.7710 - val_loss: 0.0428 - val_mean_absolute_percentage_error: 29.7194\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 0s 513us/sample - loss: 0.0400 - mean_absolute_percentage_error: 38.1042 - val_loss: 0.0391 - val_mean_absolute_percentage_error: 28.3286\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 0s 316us/sample - loss: 0.0386 - mean_absolute_percentage_error: 38.0942 - val_loss: 0.0366 - val_mean_absolute_percentage_error: 26.9823\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 0s 486us/sample - loss: 0.0382 - mean_absolute_percentage_error: 37.7166 - val_loss: 0.0350 - val_mean_absolute_percentage_error: 25.1924\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 0s 315us/sample - loss: 0.0383 - mean_absolute_percentage_error: 36.7825 - val_loss: 0.0340 - val_mean_absolute_percentage_error: 23.8238\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 0s 378us/sample - loss: 0.0387 - mean_absolute_percentage_error: 37.1681 - val_loss: 0.0332 - val_mean_absolute_percentage_error: 24.0014\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 0s 434us/sample - loss: 0.0383 - mean_absolute_percentage_error: 37.0718 - val_loss: 0.0344 - val_mean_absolute_percentage_error: 24.1854\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 0s 342us/sample - loss: 0.0385 - mean_absolute_percentage_error: 36.3056 - val_loss: 0.0346 - val_mean_absolute_percentage_error: 25.2658\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 0s 374us/sample - loss: 0.0383 - mean_absolute_percentage_error: 38.0006 - val_loss: 0.0333 - val_mean_absolute_percentage_error: 26.1922\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 0s 346us/sample - loss: 0.0383 - mean_absolute_percentage_error: 39.7104 - val_loss: 0.0337 - val_mean_absolute_percentage_error: 25.8211\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 0s 430us/sample - loss: 0.0380 - mean_absolute_percentage_error: 37.4499 - val_loss: 0.0344 - val_mean_absolute_percentage_error: 25.4031\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 0s 358us/sample - loss: 0.0384 - mean_absolute_percentage_error: 38.0059 - val_loss: 0.0348 - val_mean_absolute_percentage_error: 25.0001\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 0s 390us/sample - loss: 0.0380 - mean_absolute_percentage_error: 36.9923 - val_loss: 0.0342 - val_mean_absolute_percentage_error: 25.0425\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 0s 434us/sample - loss: 0.0378 - mean_absolute_percentage_error: 37.0419 - val_loss: 0.0327 - val_mean_absolute_percentage_error: 25.3978\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 0s 643us/sample - loss: 0.0388 - mean_absolute_percentage_error: 38.7940 - val_loss: 0.0324 - val_mean_absolute_percentage_error: 26.1153\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 0s 321us/sample - loss: 0.0390 - mean_absolute_percentage_error: 40.1630 - val_loss: 0.0325 - val_mean_absolute_percentage_error: 25.6971\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 0s 515us/sample - loss: 0.0379 - mean_absolute_percentage_error: 36.4078 - val_loss: 0.0364 - val_mean_absolute_percentage_error: 24.6037\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 0s 467us/sample - loss: 0.0408 - mean_absolute_percentage_error: 35.5902 - val_loss: 0.0399 - val_mean_absolute_percentage_error: 24.1493\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 0s 340us/sample - loss: 0.0384 - mean_absolute_percentage_error: 33.4111 - val_loss: 0.0369 - val_mean_absolute_percentage_error: 25.8302\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 0s 319us/sample - loss: 0.0391 - mean_absolute_percentage_error: 35.9343 - val_loss: 0.0342 - val_mean_absolute_percentage_error: 27.1871\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 0s 489us/sample - loss: 0.0389 - mean_absolute_percentage_error: 40.0640 - val_loss: 0.0337 - val_mean_absolute_percentage_error: 26.4252\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 0s 448us/sample - loss: 0.0382 - mean_absolute_percentage_error: 35.1920 - val_loss: 0.0348 - val_mean_absolute_percentage_error: 23.8605\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 0s 427us/sample - loss: 0.0386 - mean_absolute_percentage_error: 35.7508 - val_loss: 0.0346 - val_mean_absolute_percentage_error: 23.1458\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 0s 555us/sample - loss: 0.0389 - mean_absolute_percentage_error: 35.9614 - val_loss: 0.0331 - val_mean_absolute_percentage_error: 23.8052\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 0s 791us/sample - loss: 0.0383 - mean_absolute_percentage_error: 37.4260 - val_loss: 0.0325 - val_mean_absolute_percentage_error: 25.1113\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 479us/sample - loss: 0.0387 - mean_absolute_percentage_error: 39.3884 - val_loss: 0.0336 - val_mean_absolute_percentage_error: 26.5562\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 0s 283us/sample - loss: 0.0383 - mean_absolute_percentage_error: 39.6687 - val_loss: 0.0348 - val_mean_absolute_percentage_error: 25.9312\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 0s 552us/sample - loss: 0.0385 - mean_absolute_percentage_error: 36.9558 - val_loss: 0.0391 - val_mean_absolute_percentage_error: 25.3349\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 0s 445us/sample - loss: 0.0385 - mean_absolute_percentage_error: 34.6966 - val_loss: 0.0387 - val_mean_absolute_percentage_error: 25.4161\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 0s 427us/sample - loss: 0.0387 - mean_absolute_percentage_error: 35.6790 - val_loss: 0.0381 - val_mean_absolute_percentage_error: 25.6630\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 0s 635us/sample - loss: 0.0389 - mean_absolute_percentage_error: 36.8227 - val_loss: 0.0343 - val_mean_absolute_percentage_error: 26.4606\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 0s 301us/sample - loss: 0.0385 - mean_absolute_percentage_error: 38.3192 - val_loss: 0.0338 - val_mean_absolute_percentage_error: 25.7656\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 0s 314us/sample - loss: 0.0383 - mean_absolute_percentage_error: 38.2672 - val_loss: 0.0329 - val_mean_absolute_percentage_error: 25.9059\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 0s 366us/sample - loss: 0.0386 - mean_absolute_percentage_error: 40.0281 - val_loss: 0.0335 - val_mean_absolute_percentage_error: 25.2525\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 0s 450us/sample - loss: 0.0380 - mean_absolute_percentage_error: 37.9344 - val_loss: 0.0343 - val_mean_absolute_percentage_error: 25.6456\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 0s 381us/sample - loss: 0.0381 - mean_absolute_percentage_error: 37.6608 - val_loss: 0.0344 - val_mean_absolute_percentage_error: 25.4231\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 0s 352us/sample - loss: 0.0381 - mean_absolute_percentage_error: 38.4670 - val_loss: 0.0351 - val_mean_absolute_percentage_error: 25.9096\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 0s 444us/sample - loss: 0.0378 - mean_absolute_percentage_error: 36.7248 - val_loss: 0.0367 - val_mean_absolute_percentage_error: 25.4420\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 0s 362us/sample - loss: 0.0385 - mean_absolute_percentage_error: 34.7647 - val_loss: 0.0376 - val_mean_absolute_percentage_error: 25.1381\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 0s 377us/sample - loss: 0.0383 - mean_absolute_percentage_error: 36.9552 - val_loss: 0.0355 - val_mean_absolute_percentage_error: 25.8688\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 0s 606us/sample - loss: 0.0383 - mean_absolute_percentage_error: 37.3258 - val_loss: 0.0348 - val_mean_absolute_percentage_error: 25.6932\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 0s 679us/sample - loss: 0.0378 - mean_absolute_percentage_error: 38.8731 - val_loss: 0.0339 - val_mean_absolute_percentage_error: 26.3484\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 0s 542us/sample - loss: 0.0386 - mean_absolute_percentage_error: 38.6299 - val_loss: 0.0340 - val_mean_absolute_percentage_error: 26.0864\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 0s 466us/sample - loss: 0.0382 - mean_absolute_percentage_error: 38.2498 - val_loss: 0.0358 - val_mean_absolute_percentage_error: 26.5371\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 0s 394us/sample - loss: 0.0383 - mean_absolute_percentage_error: 36.6419 - val_loss: 0.0383 - val_mean_absolute_percentage_error: 26.7915\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 0s 281us/sample - loss: 0.0386 - mean_absolute_percentage_error: 37.9064 - val_loss: 0.0372 - val_mean_absolute_percentage_error: 26.7761\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 0s 313us/sample - loss: 0.0381 - mean_absolute_percentage_error: 37.4475 - val_loss: 0.0386 - val_mean_absolute_percentage_error: 26.0508\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 0s 340us/sample - loss: 0.0385 - mean_absolute_percentage_error: 36.6396 - val_loss: 0.0381 - val_mean_absolute_percentage_error: 26.0041\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 0s 354us/sample - loss: 0.0381 - mean_absolute_percentage_error: 36.1364 - val_loss: 0.0374 - val_mean_absolute_percentage_error: 25.0330\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 0s 316us/sample - loss: 0.0394 - mean_absolute_percentage_error: 34.7753 - val_loss: 0.0359 - val_mean_absolute_percentage_error: 24.3590\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 0s 351us/sample - loss: 0.0392 - mean_absolute_percentage_error: 40.4969 - val_loss: 0.0334 - val_mean_absolute_percentage_error: 26.4004\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 0s 266us/sample - loss: 0.0385 - mean_absolute_percentage_error: 40.0774 - val_loss: 0.0350 - val_mean_absolute_percentage_error: 26.5969\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 0s 266us/sample - loss: 0.0384 - mean_absolute_percentage_error: 38.8346 - val_loss: 0.0367 - val_mean_absolute_percentage_error: 27.2208\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 0s 345us/sample - loss: 0.0381 - mean_absolute_percentage_error: 36.1510 - val_loss: 0.0382 - val_mean_absolute_percentage_error: 26.2391\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 0s 310us/sample - loss: 0.0381 - mean_absolute_percentage_error: 36.5793 - val_loss: 0.0377 - val_mean_absolute_percentage_error: 25.3729\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 0s 245us/sample - loss: 0.0382 - mean_absolute_percentage_error: 35.7739 - val_loss: 0.0356 - val_mean_absolute_percentage_error: 24.6059\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 0s 336us/sample - loss: 0.0389 - mean_absolute_percentage_error: 36.7019 - val_loss: 0.0341 - val_mean_absolute_percentage_error: 24.1758\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 0s 326us/sample - loss: 0.0389 - mean_absolute_percentage_error: 39.9806 - val_loss: 0.0321 - val_mean_absolute_percentage_error: 25.8605\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 0s 321us/sample - loss: 0.0392 - mean_absolute_percentage_error: 40.6294 - val_loss: 0.0324 - val_mean_absolute_percentage_error: 25.0096\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 0s 493us/sample - loss: 0.0384 - mean_absolute_percentage_error: 38.1012 - val_loss: 0.0324 - val_mean_absolute_percentage_error: 25.0668\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 0s 363us/sample - loss: 0.0381 - mean_absolute_percentage_error: 38.6987 - val_loss: 0.0339 - val_mean_absolute_percentage_error: 25.1934\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 0s 692us/sample - loss: 0.0380 - mean_absolute_percentage_error: 37.3189 - val_loss: 0.0368 - val_mean_absolute_percentage_error: 26.1224\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 0s 432us/sample - loss: 0.0380 - mean_absolute_percentage_error: 36.2938 - val_loss: 0.0395 - val_mean_absolute_percentage_error: 26.2295\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 0s 409us/sample - loss: 0.0392 - mean_absolute_percentage_error: 34.5371 - val_loss: 0.0399 - val_mean_absolute_percentage_error: 25.5316\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0324 - mean_absolute_percentage_error: 26.421 - 0s 639us/sample - loss: 0.0383 - mean_absolute_percentage_error: 35.8616 - val_loss: 0.0366 - val_mean_absolute_percentage_error: 26.7241\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 0s 330us/sample - loss: 0.0391 - mean_absolute_percentage_error: 43.0183 - val_loss: 0.0330 - val_mean_absolute_percentage_error: 27.1469\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 0s 311us/sample - loss: 0.0393 - mean_absolute_percentage_error: 38.3156 - val_loss: 0.0333 - val_mean_absolute_percentage_error: 25.4136\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 269us/sample - loss: 0.0382 - mean_absolute_percentage_error: 38.2356 - val_loss: 0.0336 - val_mean_absolute_percentage_error: 24.2955\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 0s 323us/sample - loss: 0.0383 - mean_absolute_percentage_error: 37.5582 - val_loss: 0.0322 - val_mean_absolute_percentage_error: 24.5968\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 0s 434us/sample - loss: 0.0382 - mean_absolute_percentage_error: 38.6822 - val_loss: 0.0326 - val_mean_absolute_percentage_error: 24.9256\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 0s 242us/sample - loss: 0.0383 - mean_absolute_percentage_error: 38.7822 - val_loss: 0.0335 - val_mean_absolute_percentage_error: 25.0616\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 0s 341us/sample - loss: 0.0384 - mean_absolute_percentage_error: 37.8141 - val_loss: 0.0352 - val_mean_absolute_percentage_error: 26.0367\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 0s 362us/sample - loss: 0.0380 - mean_absolute_percentage_error: 37.8495 - val_loss: 0.0355 - val_mean_absolute_percentage_error: 26.4154\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 0s 280us/sample - loss: 0.0381 - mean_absolute_percentage_error: 37.7203 - val_loss: 0.0380 - val_mean_absolute_percentage_error: 26.1092\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 0s 540us/sample - loss: 0.0383 - mean_absolute_percentage_error: 36.9738 - val_loss: 0.0367 - val_mean_absolute_percentage_error: 26.2336\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 0s 357us/sample - loss: 0.0383 - mean_absolute_percentage_error: 35.9662 - val_loss: 0.0377 - val_mean_absolute_percentage_error: 26.3990\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 0s 434us/sample - loss: 0.0383 - mean_absolute_percentage_error: 36.3477 - val_loss: 0.0347 - val_mean_absolute_percentage_error: 26.3520\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 0s 384us/sample - loss: 0.0383 - mean_absolute_percentage_error: 39.0165 - val_loss: 0.0355 - val_mean_absolute_percentage_error: 25.4411\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 0s 663us/sample - loss: 0.0382 - mean_absolute_percentage_error: 36.9817 - val_loss: 0.0333 - val_mean_absolute_percentage_error: 24.9613\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 0s 563us/sample - loss: 0.0384 - mean_absolute_percentage_error: 37.4047 - val_loss: 0.0329 - val_mean_absolute_percentage_error: 23.6953\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 0s 601us/sample - loss: 0.0398 - mean_absolute_percentage_error: 40.2231 - val_loss: 0.0306 - val_mean_absolute_percentage_error: 24.2489\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 0s 603us/sample - loss: 0.0385 - mean_absolute_percentage_error: 38.3020 - val_loss: 0.0326 - val_mean_absolute_percentage_error: 23.9428\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 0s 654us/sample - loss: 0.0395 - mean_absolute_percentage_error: 35.1806 - val_loss: 0.0346 - val_mean_absolute_percentage_error: 23.5259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a31f71dd8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(x=standardized_X_train, \n",
    "          y=standardized_y_train,\n",
    "          validation_data=(standardized_X_val, standardized_y_val),\n",
    "          epochs=NUM_EPOCHS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          shuffle=SHUFFLE,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pNB5k_MIoPGv"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "pred_train = model.predict(standardized_X_train)\n",
    "pred_test = model.predict(standardized_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f8jVRQhuoPEI",
    "outputId": "591d72a8-8f52-4b3d-cfa8-c6bba55f04ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_MSE: 0.03, test_MSE: 2.25\n"
     ]
    }
   ],
   "source": [
    "# Train and test MSE\n",
    "train_mse = np.mean((standardized_y_train - pred_train) ** 2)\n",
    "test_mse = np.mean((standardized_y_test - pred_test) ** 2)\n",
    "print (f\"train_MSE: {train_mse:.2f}, test_MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-4X3GZEdoR-8",
    "outputId": "eea237a4-a2b3-4d40-f24f-6c64082fda37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[actual] y = 3.5X + noise\n",
      "[model] y_hat = 3.4X + 8.0\n"
     ]
    }
   ],
   "source": [
    "# Unstandardize coefficients (proof is in the `From Scratch` section above)\n",
    "W = model.layers[0].get_weights()[0][0][0]\n",
    "b = model.layers[0].get_weights()[1][0]\n",
    "W_unscaled = W * (y_scaler.scale_/X_scaler.scale_)\n",
    "b_unscaled = b * y_scaler.scale_ + y_scaler.mean_ - np.sum(W_unscaled*X_scaler.mean_)\n",
    "print (\"[actual] y = 3.5X + noise\")\n",
    "print (f\"[model] y_hat = {W_unscaled[0]:.1f}X + {b_unscaled[0]:.1f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdNX2W5eh2ma"
   },
   "source": [
    "Regularization didn't help much with this specific example because our data is generated from a perfect linear equation but for large realistic data, regularization can help our model generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V74lNFE5v5pQ"
   },
   "source": [
    "# Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2r6Xhyg7v5vX"
   },
   "source": [
    "In our example, the feature was a continuous variable but what if we also have features that are categorical? One option is to treat the categorical variables as one-hot encoded variables. This is very easy to do with Pandas and once you create the dummy variables, you can use the same steps as above to train your linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "unhcIOfMxQEQ",
    "outputId": "458bfea8-d946-4cd6-cf1c-5943a952008c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_letter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  favorite_letter\n",
       "0               a\n",
       "1               b\n",
       "2               c\n",
       "3               a"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data with categorical features\n",
    "cat_data = pd.DataFrame(['a', 'b', 'c', 'a'], columns=['favorite_letter'])\n",
    "cat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "m4eQmJdrxQGr",
    "outputId": "b6bf3b36-c210-4587-b73a-617c9da6f6ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_letter_a</th>\n",
       "      <th>favorite_letter_b</th>\n",
       "      <th>favorite_letter_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   favorite_letter_a  favorite_letter_b  favorite_letter_c\n",
       "0                  1                  0                  0\n",
       "1                  0                  1                  0\n",
       "2                  0                  0                  1\n",
       "3                  1                  0                  0"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_cat_data = pd.get_dummies(cat_data)\n",
    "dummy_cat_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5R8x-KyiBWJ"
   },
   "source": [
    "Now you can concat this with your continuous features and train the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XwSZgi_pCb3Q"
   },
   "source": [
    "---\n",
    "<div align=\"center\">\n",
    "\n",
    "Subscribe to our <a href=\"https://practicalai.me/#newsletter\">newsletter</a> and follow us on social media to get the latest updates!\n",
    "\n",
    "<a class=\"ai-header-badge\" target=\"_blank\" href=\"https://github.com/practicalAI/practicalAI\">\n",
    "              <img src=\"https://img.shields.io/github/stars/practicalAI/practicalAI.svg?style=social&label=Star\"></a>&nbsp;\n",
    "            <a class=\"ai-header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/company/practicalai-me\">\n",
    "              <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n",
    "            <a class=\"ai-header-badge\" target=\"_blank\" href=\"https://twitter.com/practicalAIme\">\n",
    "              <img src=\"https://img.shields.io/twitter/follow/practicalAIme.svg?label=Follow&style=social\">\n",
    "            </a>\n",
    "              </div>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sdruDHf_laWg"
   ],
   "name": "04_Linear_Regression",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
